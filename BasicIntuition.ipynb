{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OptimoCX/BootCampIA/blob/main/BasicIntuition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Intuition in Transformers"
      ],
      "metadata": {
        "id": "4L3v5GubivCk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi9Ue5KChoLn",
        "outputId": "1babbb2e-871c-4b5f-d785-e9c63d179e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 - Tokenized Words: ['Awareness', 'is', 'power', 'in', 'a', 'world', 'where', 'knowledge', 'is', 'everywhere']\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Tokenize the sentence\n",
        "sentence = \"Awareness is power in a world where knowledge is everywhere\"\n",
        "words = sentence.split()\n",
        "print(f\"Step 1 - Tokenized Words: {words}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Simulate embeddings (simple numerical representations)\n",
        "word_embeddings = {word: [ord(char) - 96 for char in word.lower()] for word in words}\n",
        "\n",
        "print(\"\\nStep 2 - Word Embeddings:\")\n",
        "for word, embedding in word_embeddings.items():\n",
        "    print(f\"{word}: {embedding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S8fgjQ5hqme",
        "outputId": "f56c9cd9-fe19-4de5-ed4e-bb7086cf55d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2 - Word Embeddings:\n",
            "Awareness: [1, 23, 1, 18, 5, 14, 5, 19, 19]\n",
            "is: [9, 19]\n",
            "power: [16, 15, 23, 5, 18]\n",
            "in: [9, 14]\n",
            "a: [1]\n",
            "world: [23, 15, 18, 12, 4]\n",
            "where: [23, 8, 5, 18, 5]\n",
            "knowledge: [11, 14, 15, 23, 12, 5, 4, 7, 5]\n",
            "everywhere: [5, 22, 5, 18, 25, 23, 8, 5, 18, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Automatically generate attention weights based on word length\n",
        "total_characters = sum(len(word) for word in words)\n",
        "attention_weights = {word: len(word) / total_characters for word in words}\n",
        "\n",
        "print(\"\\nStep 3 - Attention Weights:\")\n",
        "for word, weight in attention_weights.items():\n",
        "    print(f\"{word}: {weight:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR4U4Eldhqhf",
        "outputId": "cb417c28-80f1-4c5e-a773-aa4c23910d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3 - Attention Weights:\n",
            "Awareness: 0.180\n",
            "is: 0.040\n",
            "power: 0.100\n",
            "in: 0.040\n",
            "a: 0.020\n",
            "world: 0.100\n",
            "where: 0.100\n",
            "knowledge: 0.180\n",
            "everywhere: 0.200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Compute weighted sum of embeddings\n",
        "weighted_embeddings = {word: [weight * val for val in embedding]\n",
        "                       for word, embedding in word_embeddings.items()\n",
        "                       for word_weight, weight in attention_weights.items() if word == word_weight}\n",
        "\n",
        "final_vector = [0] * len(max(word_embeddings.values(), key=len))\n",
        "for embedding in weighted_embeddings.values():\n",
        "    final_vector = [sum(x) for x in zip(final_vector, embedding)]\n",
        "\n",
        "print(\"\\nStep 4 - Final Vector Before Transformation:\")\n",
        "print(final_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9USR3IOhqdj",
        "outputId": "c7d87d4f-1718-4091-e894-8c3c9502925f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4 - Final Vector Before Transformation:\n",
            "[10.100000000000001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: Apply a simple transformation as a simulated model process\n",
        "processed_output = sum(final_vector)\n",
        "\n",
        "print(f\"\\nStep 5 - Processed Output: {processed_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGvrp_YahqY7",
        "outputId": "5f671371-3215-41af-d5b8-c7b852eebf96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 5 - Processed Output: 10.100000000000001\n"
          ]
        }
      ]
    }
  ]
}